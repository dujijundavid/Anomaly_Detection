{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c9a96d",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as spss\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "import time \n",
    "import datetime\n",
    "from time import strptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165e8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f840141",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_database_option = False\n",
    "insert_prometheus_option = False\n",
    "random_client=1\n",
    "\n",
    "\n",
    "if random_client==1:\n",
    "    # 1 year of data, every 5 minute. 24*12 = 288\n",
    "    normal_data_rownumber= 100000\n",
    "    abnormal_data_row_number=500\n",
    "    csv_file_name= \"rand1.csv\"\n",
    "    customer_name_list = [\"RAND001\"]\n",
    "else:\n",
    "    normal_data_rownumber= 19000\n",
    "    abnormal_data_row_number=1000\n",
    "    customer_name_list = [\"RAND002\"]\n",
    "    csv_file_name= \"rand2.csv\"\n",
    "\n",
    "total_row_number = normal_data_rownumber+abnormal_data_row_number*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afdf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start,time_end = '2022-01-01 00:00:00', '2022-12-31 23:59:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd7166",
   "metadata": {},
   "source": [
    "# Continuous Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f6195",
   "metadata": {},
   "source": [
    "## Uniform Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_list = spss.gamma.rvs(a=40000, size=normal_data_rownumber).tolist() # size specifies number of random variates, a is the shape parameter\n",
    "gamma_list= [int(i) for i in gamma_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "a = 48000\n",
    "gamma.ppf([0.5, 0.9,0.95, 0.999], a)\n",
    "gamma_list= gamma.rvs(a, size=normal_data_rownumber)\n",
    "sns.distplot(gamma_list, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f2138",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_list = spss.uniform.rvs(size=normal_data_rownumber, loc = 0, scale=10).tolist()\n",
    "normal_list = spss.norm.rvs(size=normal_data_rownumber,loc=5,scale=1).tolist()\n",
    "\n",
    "\n",
    "# gamma_list = spss.gamma.rvs(a=5, size=normal_data_rownumber).tolist() # size specifies number of random variates, a is the shape parameter\n",
    "# gamma_list= [i/2 for i in gamma_list]\n",
    "#exponential_list = spss.expon.rvs(scale=1,loc=45000,size=normal_data_rownumber).tolist() # size specifies number of random variates, loc corresponds to mean, scale corresponds to standard deviation\n",
    "\n",
    "# Generate data for Peak download_speed\n",
    "exponential_list = spss.expon.rvs(scale=1000,loc=18000,size=normal_data_rownumber).tolist() # size specifies number of random variates, loc corresponds to mean, scale corresponds to standard deviation\n",
    "exponential_list = [int(np.round(item)) for item in exponential_list]\n",
    "\n",
    "exponential_list2 = spss.expon.rvs(scale=1000,loc=45000,size=normal_data_rownumber).tolist() # size specifies number of random variates, loc corresponds to mean, scale corresponds to standard deviation\n",
    "exponential_list2 = [int(np.round(item)) for item in exponential_list2]\n",
    "\n",
    "\n",
    "poisson_list = spss.poisson.rvs(mu=3, size=normal_data_rownumber).tolist() # size specifies number of random variates, loc corresponds to mean, scale corresponds to standard deviation\n",
    "\n",
    "\n",
    "\n",
    "binomial_list = spss.binom.rvs(n=10,p=0.8,size=normal_data_rownumber).tolist() # n = number of trials, p = probability of success, size = number of times to repeat the trials\n",
    "bernoulli_list = spss.bernoulli.rvs(size=normal_data_rownumber,p=0.6).tolist() # p = probability of success, size = number of times to repeat the trial\n",
    "bernoulli_list = [i*10 for i in bernoulli_list]\n",
    "\n",
    "continuous_distribution = [uniform_list,normal_list,exponential_list2,exponential_list]\n",
    "discrete_distribution = [binomial_list,poisson_list,bernoulli_list]\n",
    "\n",
    "column_names = [\"uniform\",\"normal\",\"exponential_list2\",\"exponential\",\"binomial\",\"poisson\",\"bernoulli\"]\n",
    "\n",
    "continuous_df = pd.DataFrame(continuous_distribution).T.round(2)\n",
    "discrete_df =  pd.DataFrame(discrete_distribution).T\n",
    "normal_data_df = pd.concat([continuous_df,discrete_df],axis=1)\n",
    "normal_data_df.columns = column_names\n",
    "normal_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_list_abnormal_upper = spss.uniform.rvs(size=abnormal_data_row_number, loc = 10, scale=10).tolist()\n",
    "uniform_list_abnormal_lower = spss.uniform.rvs(size=abnormal_data_row_number, loc = -10, scale=10).tolist()\n",
    "\n",
    "uniform_abnormal= uniform_list_abnormal_upper+uniform_list_abnormal_lower\n",
    "\n",
    "abnormal_data= [uniform_abnormal,uniform_abnormal,uniform_abnormal,uniform_abnormal,uniform_abnormal,uniform_abnormal,uniform_abnormal]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_data_row_number*=2\n",
    "uniform_list_abnormal = spss.uniform.rvs(size=abnormal_data_row_number, loc = 5, scale=10).tolist()\n",
    "normal_list_abnormal = spss.norm.rvs(size=abnormal_data_row_number,loc=10,scale=1).tolist()\n",
    "\n",
    "exponential_list_abnormal = spss.expon.rvs(scale=1000,loc=22000,size=abnormal_data_row_number).tolist() # size specifies number of random variates, loc corresponds to mean, scale corresponds to standard deviation\n",
    "exponential_list_abnormal = [int(np.round(item)) for item in exponential_list_abnormal]\n",
    "\n",
    "exponential_list_abnormal2 = spss.expon.rvs(scale=1000,loc=50000,size=abnormal_data_row_number).tolist() # size specifies number of random variates, loc corresponds to mean, scale corresponds to standard deviation\n",
    "exponential_list_abnormal2 = [int(np.round(item)) for item in exponential_list_abnormal2]\n",
    "\n",
    "\n",
    "poisson_list_abnormal = spss.poisson.rvs(mu=8, size=abnormal_data_row_number).tolist() # size specifies number of random variates, loc corresponds to mean, scale corresponds to standard deviation\n",
    "\n",
    "\n",
    "\n",
    "binomial_list_abnormal = spss.binom.rvs(n=15,p=0.8,size=abnormal_data_row_number).tolist() # n = number of trials, p = probability of success, size = number of times to repeat the trials\n",
    "bernoulli_list_abnormal = spss.bernoulli.rvs(size=abnormal_data_row_number,p=0.9).tolist() # p = probability of success, size = number of times to repeat the trial\n",
    "bernoulli_list_abnormal = [i*10 for i in bernoulli_list_abnormal]\n",
    "\n",
    "continuous_distribution_abnormal = [uniform_list_abnormal,normal_list_abnormal,exponential_list_abnormal2,exponential_list_abnormal]\n",
    "discrete_distribution_abnormal = [binomial_list_abnormal,poisson_list_abnormal,bernoulli_list_abnormal]\n",
    "\n",
    "column_names = [\"uniform\",\"normal\",\"exponential_list2\",\"exponential\",\"binomial\",\"poisson\",\"bernoulli\"]\n",
    "\n",
    "continuous_df_abnormal = pd.DataFrame(continuous_distribution_abnormal).T.round(2)\n",
    "discrete_df_abnormal =  pd.DataFrame(discrete_distribution_abnormal).T\n",
    "abnormal_data_df = pd.concat([continuous_df_abnormal,discrete_df_abnormal],axis=1)\n",
    "abnormal_data_df.columns = column_names\n",
    "abnormal_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_data_df=abnormal_data_df.round(0)\n",
    "final_data =  pd.concat([normal_data_df ,abnormal_data_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39bed80",
   "metadata": {},
   "source": [
    "\n",
    "# Combine them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.sample(frac=1).reset_index(drop=True)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bfb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform generated Data into Gyan-core-stats format\n",
    "\n",
    "Things need to do:\n",
    "\n",
    "## 1. Generate 12-7 = 5 More columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_list1 = spss.bernoulli.rvs(size=total_row_number,p=0.1).tolist() # p = probability of success, size = number of times to repeat the trial\n",
    "bernoulli_list2 = spss.bernoulli.rvs(size=total_row_number,p=0.2).tolist() # p = probability of success, size = number of times to repeat the trial\n",
    "bernoulli_list3 = spss.bernoulli.rvs(size=total_row_number,p=0.3).tolist() # p = probability of success, size = number of times to repeat the trial\n",
    "bernoulli_list4 = spss.bernoulli.rvs(size=total_row_number,p=0.4).tolist() # p = probability of success, size = number of times to repeat the trial\n",
    "bernoulli_list5 = spss.bernoulli.rvs(size=total_row_number,p=0.5).tolist() # p = probability of success, size = number of times to repeat the trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake 4 customer IDs\n",
    "\n",
    "nunique_customers= len(customer_name_list)\n",
    "mapping_list=[]\n",
    "for num in range(nunique_customers):\n",
    "    new_customer =  [customer_name_list[num] for i in range(int(total_row_number/nunique_customers))]\n",
    "    mapping_list.extend(new_customer)\n",
    "    \n",
    "assert len(mapping_list) == total_row_number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.shuffle(mapping_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd7a0d1",
   "metadata": {},
   "source": [
    "## Generate statstimestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d04086",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate timestamp\n",
    "timestamp_list = (pd.DataFrame(columns=['NULL'],\n",
    "                  index=pd.date_range(time_start, time_end,\n",
    "                                      freq='5T'))\n",
    "       .between_time('00:00','23:59')\n",
    "       .index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "       .tolist()\n",
    ")\n",
    "\n",
    "timestamp_list = timestamp_list[:total_row_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d9dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(timestamp_list) == final_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data= final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71826074",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data[\"bernoulli_list1\"] =bernoulli_list1\n",
    "last_data[\"bernoulli_list2\"] =bernoulli_list2\n",
    "last_data[\"bernoulli_list3\"] =bernoulli_list3\n",
    "last_data[\"bernoulli_list4\"] =bernoulli_list4\n",
    "last_data[\"bernoulli_list5\"] =bernoulli_list5\n",
    "last_data[\"mapping_list\"]=mapping_list\n",
    "last_data[\"timestamp_list\"]=timestamp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8c72c",
   "metadata": {},
   "source": [
    "# Insert into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d0a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_data.columns\n",
    "\n",
    "final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_attached_user  -> \n",
    "# cell_number\n",
    "# \n",
    "final_df[\"client_id\"] = last_data[\"mapping_list\"]\n",
    "final_df[\"stats_timestamp\"] = last_data[\"timestamp_list\"]\n",
    "final_df[\"total_attached_user\"] = last_data[\"bernoulli_list5\"]\n",
    "final_df[\"total_rejected_user\"] = last_data[\"bernoulli_list4\"]\n",
    "final_df[\"peak_upload_speed\"] = last_data[\"exponential_list2\"]\n",
    "final_df[\"peak_download_speed\"] = last_data[\"exponential\"]\n",
    "final_df[\"enodeb_shutdown_count\"] = last_data[\"bernoulli_list3\"]\n",
    "final_df[\"handover_failure_count\"] = last_data[\"bernoulli_list2\"]\n",
    "final_df[\"bearer_active_user_count\"] = last_data[\"bernoulli_list1\"]\n",
    "final_df[\"bearer_rejected_user_count\"] = last_data[\"bernoulli\"]\n",
    "final_df[\"total_users\"] = last_data[\"binomial\"]\n",
    "final_df[\"total_dropped_packets\"] = last_data[\"poisson\"]\n",
    "final_df[\"enodeb_connected_count\"] = last_data[\"normal\"]\n",
    "final_df[\"enodeb_connection_status\"] = last_data[\"uniform\"]\n",
    "\n",
    "final_df.to_csv(csv_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(csv_file_name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085ee6f",
   "metadata": {},
   "source": [
    "## Connect with Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "# Initiate with Parameters\n",
    "db_name = \"core_stats\"\n",
    "col = \"peak_upload_speed\"\n",
    "\n",
    "\n",
    "# Start Database Connection\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=\"10.1.2.10\",\n",
    "    user=\"gyan\",\n",
    "    password=\"5Gaa$2022\",\n",
    "    database=\"gyan_db\"\n",
    ")\n",
    "\n",
    "# Load data from database and store as pandas Dataframe\n",
    "df_rand = pd.read_sql(\n",
    "    'SELECT * FROM gyan_db.core_stats WHERE client_id= \"BETBEL01GYN001\" AND stats_timestamp>\"2022-07-15\"'.format(db_name), con=db_connection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7681e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rand.to_csv(\"rand_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(10, 10)}) #width=3, #height=4\n",
    "\n",
    "sns.distplot(exponential_list, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Existing_rand_data_after_July15.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd1b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\":(30, 10)}) #width=3, #height=4\n",
    "\n",
    "final_df.peak_upload_speed.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade95b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"reverse_peak_upload_speed\"]= 100000-final_df.peak_upload_speed\n",
    "final_df.reverse_peak_upload_speed.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058eb557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_time_series(time, values, label):\n",
    "    plt.figure(figsize=(10,6),dpi=80)\n",
    "    \n",
    "    plt.plot(time, values)\n",
    "    plt.xlabel(\"Time\", fontsize=20)\n",
    "    plt.ylabel(\"Value\", fontsize=20)\n",
    "    plt.title(label, fontsize=20)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6ece5",
   "metadata": {},
   "source": [
    "## Trend+Seasonality+Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5941623",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_time = np.arange(total_row_number)\n",
    "trend_values = trend_time*0.001\n",
    "plot_time_series(trend_time, trend_values, \"Upward Trend\")\n",
    "\n",
    "\n",
    "# Just a random pattern\n",
    "time = np.arange(total_row_number)\n",
    "values = np.where(time < 3000, time**(1/5), (time)**(1/6))\n",
    "\n",
    "# Repeat the pattern 5 times\n",
    "seasonal = []\n",
    "for i in range(5):\n",
    "    for j in range(int(total_row_number/5)):\n",
    "        seasonal.append(values[j])\n",
    "# Plot\n",
    "time_seasonal = np.arange(total_row_number)\n",
    "plot_time_series(time_seasonal, seasonal, label=\"Seasonality\")\n",
    "\n",
    "\n",
    "noise = np.random.randn(total_row_number)*0.05\n",
    "seasonal += noise\n",
    "time_seasonal = np.arange(total_row_number)\n",
    "plot_time_series(time_seasonal, seasonal, label=\"Seasonality with Noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_upward = seasonal + np.arange(total_row_number)*0.0001\n",
    "\n",
    "plot_time_series(trend_values, seasonal_upward, label=\"Seasonality + Upward Trend + Noise\")\n",
    "final_df_copy= final_df.copy()\n",
    "final_df_copy[\"peak_upload_speed\"]= final_df_copy[\"peak_upload_speed\"]+seasonal_upward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2283db",
   "metadata": {},
   "source": [
    "## Peak Upload Speed i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064534b4",
   "metadata": {},
   "source": [
    "### Peak Upload Speed in New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(30, 19), dpi=80)\n",
    "final_df_copy[\"peak_upload_speed\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de768ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1= final_df_copy.peak_upload_speed[:int(final_df_copy.shape[0]/4)]\n",
    "figure(figsize=(30, 19), dpi=80)\n",
    "\n",
    "s1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7031c",
   "metadata": {},
   "source": [
    "### Peak Upload Speed in Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(30, 19), dpi=80)\n",
    "\n",
    "df_rand[\"peak_upload_speed\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_row_number= 40000\n",
    "trend_time = np.arange(total_row_number)\n",
    "trend_values = trend_time*0.001\n",
    "plot_time_series(trend_time, trend_values, \"Upward Trend\")\n",
    "\n",
    "\n",
    "# Just a random pattern\n",
    "time = np.arange(total_row_number)\n",
    "values = np.where(time < 3000, time**(1/5), (time)**(1/6))\n",
    "\n",
    "# Repeat the pattern 5 times\n",
    "seasonal = []\n",
    "for i in range(5):\n",
    "    for j in range(int(total_row_number/5)):\n",
    "        seasonal.append(values[j])\n",
    "# Plot\n",
    "time_seasonal = np.arange(total_row_number)\n",
    "plot_time_series(time_seasonal, seasonal, label=\"Seasonality\")\n",
    "\n",
    "\n",
    "noise = np.random.randn(total_row_number)*0.05\n",
    "seasonal += noise\n",
    "time_seasonal = np.arange(total_row_number)\n",
    "plot_time_series(time_seasonal, seasonal, label=\"Seasonality with Noise\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling to Detect Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------------------------------------------------------------------------\n",
    "#---------------------------------Part 1: DB connection--------------------------------------\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from scipy.stats import iqr\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import mysql.connector\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_Z_score_column(df,col, z_score_threshold=3):\n",
    "    col_name =  \"Z-score_\" + col\n",
    "    Z_score_list = (df[col] - df[col].mean())/df[col].std(ddof=0)\n",
    "    Z_score_list= [np.round(x,2) for x in Z_score_list]\n",
    "    df[col_name]=Z_score_list\n",
    "    df[\"label_Z-score_\"+col] = [ 1 if (x< -1*z_score_threshold or x>1*z_score_threshold) else 0 for x in df[col_name]]\n",
    "    \n",
    "    deviation_list = []\n",
    "    value_list = df[col]\n",
    "    mean_value= df[col].mean()\n",
    "\n",
    "    for item in value_list:\n",
    "        if mean_value == 0:\n",
    "            deviation_list.append(0)\n",
    "        else:\n",
    "            deviation_list.append(np.round((item-mean_value) / mean_value ,2)) \n",
    "\n",
    "    df[\"deviation_Z-score_\"+col]=  deviation_list\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def add_outlier_column(df, col, iqr_factor=2, low_quantile=25, upper_quantile=75):\n",
    "    q3, q1 = np.percentile(df[col], [upper_quantile, low_quantile])\n",
    "\n",
    "    iqr_v = iqr(df[col])\n",
    "\n",
    "    upper_v = q3+iqr_factor*iqr_v\n",
    "    lower_v = q1-iqr_factor*iqr_v\n",
    "    \n",
    "    #outliers_removed = [x for x in df[col] if x >= lower_v and x <= upper_v]\n",
    "    #print(outliers_removed)\n",
    "    \n",
    "    outlier_index = df[(df[col] > upper_v) | (df[col] < lower_v)][col].index\n",
    "    outlier_label = [1 if x in outlier_index else 0 for x in df.index]\n",
    "    \n",
    "    df[\"label_outlier_\"+col]  = outlier_label\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def reorder_columns(dataframe, col_name, position):\n",
    "    \"\"\"Reorder a dataframe's column.\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): dataframe to use\n",
    "        col_name (string): column name to move\n",
    "        position (0-indexed position): where to relocate column to\n",
    "    Returns:\n",
    "        pd.DataFrame: re-assigned dataframe\n",
    "    \"\"\"\n",
    "    temp_col = dataframe[col_name]\n",
    "    dataframe = dataframe.drop(columns=[col_name])\n",
    "    dataframe.insert(loc=position, column=col_name, value=temp_col)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\n",
    "def insert_anomaly_to_database(current_anomlies):\n",
    "    for index, row in current_anomlies.iterrows():\n",
    "        connection = mysql.connector.connect(\n",
    "            host=\"10.1.2.10\",\n",
    "            user=\"gyan\",\n",
    "            password=\"5Gaa$2022\",\n",
    "            database=\"gyan_db\"\n",
    "        )\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        MySQL_insert_query = \"INSERT INTO tb_export_anomaly_df (client_id, stats_timestamp, attribute_name, attribute_value, attribute_label_Z_score, attribute_deviation, attribute_label_outlier,attribute_mean) VALUES (%s, %s, %s, %s, %s, %s, %s,%s)\"\n",
    "\n",
    "        the_value= (row.client_id, str(row.stats_timestamp),row.Attribute_Name, row.Attribute_Value, row.Attribute_Label_Z_Score, row.Attribute_Deviation_Z, row.Attribute_Label_Outlier, row.attribute_mean)\n",
    "        \n",
    "        try:\n",
    "            cursor.execute(MySQL_insert_query, the_value)\n",
    "        except:\n",
    "            print(\"Record Already Inserted\")\n",
    "            pass\n",
    "\n",
    "        connection.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    print(\"Insert Complete\")\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "#---------------------------------Part 3: Data Cleaning --------------------------------------\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "keeped_column_name =list(final_df_copy.columns)\n",
    "keeped_column_name.remove(\"client_id\")\n",
    "# keeped_column_name.remove(\"total_tx_data\")\n",
    "# keeped_column_name.remove(\"total_rx_data\")\n",
    "keeped_column_name.remove(\"stats_timestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new table to store all the computed metrics\n",
    "Anomaly_table = final_df_copy.copy()\n",
    "\n",
    "## Statistics Anomalies and Outlier Anomalies\n",
    "\n",
    "temp_df = Anomaly_table\n",
    "\n",
    "for col in keeped_column_name:\n",
    "\n",
    "    temp_df=add_Z_score_column(temp_df,col)\n",
    "    temp_df=add_outlier_column(temp_df,col)\n",
    "\n",
    "Anomaly_table=Anomaly_table.append(temp_df, ignore_index = True)\n",
    "\n",
    "filter_col = [col for col in Anomaly_table if col.startswith('label')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "#---------------------------------Part 4: Prepare Export Anomaly Dataframe-----------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "# folder_path = \"C:\\Users\\Jijun Du\\Desktop\\Main_Anomaly_Detection\\Generated_csv\\\"\n",
    "# Filter columns with at least one record that have anomaly label\n",
    "export_anomaly_df= pd.DataFrame()\n",
    "for col in keeped_column_name:\n",
    "    condition= (Anomaly_table[\"label_Z-score_\"+ col]==1) | (Anomaly_table[\"label_outlier_\"+col]==1)\n",
    "\n",
    "\n",
    "    subset_columns=[\"client_id\",\"stats_timestamp\",col,\"label_Z-score_\"+col, \"deviation_Z-score_\"+col,\"label_outlier_\"+col]\n",
    "    #print(\"{} rows of anomaly detected for column {}\".format(sum(condition),col))\n",
    "    \n",
    "    subset_Summary= Anomaly_table[condition][subset_columns]\n",
    "    \n",
    "    subset_Summary[\"Attribute_Name\"] = col\n",
    "    \n",
    "    subset_Summary = reorder_columns(subset_Summary,\"Attribute_Name\",2)\n",
    "    #subset_Summary.drop(col, axis=1)\n",
    "#print(subset_Summary.shape[1])\n",
    "#print(subset_Summary.columns)\n",
    "    subset_Summary=subset_Summary.rename(columns={str(col):\"Attribute_Value\",\"label_Z-score_\"+col: \"Attribute_Label_Z_Score\",  \"deviation_Z-score_\"+col: \"Attribute_Deviation_Z\",\"label_outlier_\"+col: \"Attribute_Label_Outlier\"})\n",
    "    \n",
    "    export_anomaly_df=export_anomaly_df.append(subset_Summary, ignore_index = True)\n",
    "\n",
    "\n",
    "    \n",
    "mean_list= []\n",
    "for i in range(export_anomaly_df.shape[0]):\n",
    "    try:\n",
    "\n",
    "        attribute_condition = export_anomaly_df.iloc[i].Attribute_Name\n",
    "        client_condition = mean_summary[\"client_id\"] == export_anomaly_df.iloc[i].client_id\n",
    "        value= mean_summary[client_condition][attribute_condition].values[0]\n",
    "        mean_list.append(value)\n",
    "    except:\n",
    "        mean_list.append(0)\n",
    "        \n",
    "export_anomaly_df[\"attribute_mean\"]= mean_list\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "#---------------------------------Part 5: Get Current Anomalies and Insert-----------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "time_interval = datetime.datetime.now() - datetime.timedelta(minutes=120)\n",
    "#current_anomlies= export_anomaly_df[export_anomaly_df.stats_timestamp > time_interval].reset_index()\n",
    "\n",
    "\n",
    "# if current_anomlies.shape[0]>0:\n",
    "#     insert_anomaly_to_database(current_anomlies)\n",
    "#     print(\"Anomalies Insert Completed\")\n",
    "# else:\n",
    "#     print(\"No Anomalies Detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992f412",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_anomaly_df[export_anomaly_df[\"Attribute_Name\"]!=\"reverse_peak_upload_speed\"].shape[0]/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01034c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"randomized_data_with_distribution_1year_V2.csv\")\n",
    "\n",
    "profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
    "\n",
    "profile.to_file(\"randomized_data_with_distribution_1year_V2.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Initiate with Parameters\n",
    "db_name = \"core_stats\"\n",
    "col = \"peak_upload_speed\"\n",
    "\n",
    "\n",
    "# Start Database Connection\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=\"10.1.2.10\",\n",
    "    user=\"gyan\",\n",
    "    password=\"5Gaa$2022\",\n",
    "    database=\"gyan_db\"\n",
    ")\n",
    "# # Create Database Cursor for SQL Queries\n",
    "# mycursor = db_connection.cursor()\n",
    "# mycursor.execute(\"SELECT * FROM {} LIMIT 5\".format(db_name))\n",
    "\n",
    "# myresult = mycursor.fetchall()\n",
    "# for x in myresult:\n",
    "#     print(x)\n",
    "\n",
    "\n",
    "# Load data from database and store as pandas Dataframe\n",
    "df = pd.read_sql(\n",
    "    'SELECT * FROM gyan_db.core_stats WHERE client_id in (\"BETBEL01GYN001\")'.format(db_name), con=db_connection)\n",
    "\n",
    "\n",
    "\n",
    "profile = ProfileReport(df, title=\"Original Randomized Data\")\n",
    "\n",
    "profile.to_file(\"randomized_data_without_distribution.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
